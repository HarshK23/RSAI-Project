{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "print(device)\n",
    "\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "# torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Importing MNIST train and test data\n",
    "train_data = datasets.CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = datasets.CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, output_dim):  # Only output_dim is needed\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet18 model and freeze its weights\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "\n",
    "        # Adjust the final layer to match your output dimension\n",
    "        # self.flatten = nn.Flatten()\n",
    "\n",
    "        self.flatten = nn.Flatten()  # Add the flatten layer\n",
    "        self.fc = nn.Linear(1000, output_dim,bias=True)  # Input size based on pre-trained model\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet18 model\n",
    "        x = self.resnet(x)\n",
    "        # print(\"X ka shape after resnet \",x.shape)\n",
    "        # Apply the final linear layer\n",
    "        # print(\"X ka shape before flatten \",x.shape)\n",
    "        x= self.flatten(x)\n",
    "        # print(\"X ka shape aafter flatten \",x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(\"X ka shape which is output\",x.shape)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc(data, model, loss_fn):\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _,preds = scores.max(1)\n",
    "            total_loss += loss_fn(scores, y).item()\n",
    "            n_samples += preds.size(0)\n",
    "            n_correct += (preds == y).sum()\n",
    "            n_batches += 1\n",
    "    acc = float(n_correct/n_samples)\n",
    "    loss = float(total_loss/n_batches)\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, loss_fn, num_epochs=5, batch_size=64):\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
    "    accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            # print(\"inside train\",x.shape)\n",
    "\n",
    "            train_preds = model(x)\n",
    "            loss = loss_fn(train_preds, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc, _ = check_acc(train_loader, model, loss_fn)\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        #save the highest accuracy model \n",
    "        \n",
    "        if train_acc > accuracy and train_acc > 0.9:\n",
    "            accuracy = train_acc\n",
    "            torch.save(model.state_dict(), 'best_model100.pth')\n",
    "\n",
    "\n",
    "def eval_model(model, test_data, loss_fn):\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    test_acc, test_loss = check_acc(test_loader, model, loss_fn)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "def eval_untargetted_fgsm(model, test_data, loss_fn, e=0.1):\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            x.requires_grad = True\n",
    "            scores = model(x)\n",
    "            loss = loss_fn(scores, y)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            x_grad = x.grad.data\n",
    "            x_adv = x + e * x_grad.sign()\n",
    "            x_adv = torch.clamp(x_adv, 0, 1)\n",
    "            out_adv = model(x_adv)\n",
    "            _,preds = out_adv.max(1)\n",
    "            total_loss += loss_fn(out_adv, y).item()\n",
    "            n_samples += preds.size(0)\n",
    "            n_correct += (preds == y).sum()\n",
    "            n_batches += 1\n",
    "    acc = float(n_correct/n_samples)\n",
    "    loss = float(total_loss/n_batches)\n",
    "    print(f'Untargetted FGSM Test Loss: {loss:.4f}, Untargetted FGSM Test Acc: {acc:.4f}')\n",
    "    return acc\n",
    "\n",
    "# write code for targetted fgsm attack\n",
    "def eval_targeted_fgsm(model, test_data, loss_fn, target_label, e=0.1):\n",
    "    test_loader = DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    target_label = target_label.to(device=device, dtype=torch.long)\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            x.requires_grad = True\n",
    "            scores = model(x)\n",
    "            # loss1 = loss_fn(scores, y)\n",
    "            # loss2 = loss_fn(scores, target_label)\n",
    "            # loss = loss1-loss2\n",
    "            loss = loss_fn(scores, target_label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            x_grad = x.grad.data\n",
    "            x_adv = x - e * x_grad.sign()\n",
    "            x_adv = torch.clamp(x_adv, 0, 1)\n",
    "            out_adv = model(x_adv)\n",
    "            _,preds = out_adv.max(1)\n",
    "            total_loss += loss_fn(out_adv, y).item()\n",
    "            n_samples += preds.size(0)\n",
    "            n_correct += (preds == y).sum()\n",
    "            n_batches += 1\n",
    "    acc = float(n_correct/n_samples)\n",
    "    loss = float(total_loss/n_batches)\n",
    "    print(f'Targetted FGSM Test Loss: {loss:.4f}, Targetted FGSM Test Acc: {acc:.4f}')\n",
    "    return acc\n",
    "\n",
    "def eval_untargetted_mim(model, test_data, loss_fn, eps=0.1, alpha=0.9):\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]))\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    momentum = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        x.requires_grad = True\n",
    "        scores = model(x)\n",
    "        loss = loss_fn(scores, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        x_grad = x.grad.data\n",
    "\n",
    "        # Update momentum and create adversarial example\n",
    "        momentum = alpha * momentum + (1 - alpha) * x_grad.sign()\n",
    "        x_adv = x + eps * momentum\n",
    "\n",
    "        # Clip adversarial example to valid range\n",
    "        x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "        out_adv = model(x_adv)\n",
    "        _, preds = out_adv.max(1)\n",
    "        total_loss += loss_fn(out_adv, y).item()\n",
    "        n_samples += preds.size(0)\n",
    "        n_correct += (preds == y).sum()\n",
    "        n_batches += 1\n",
    "\n",
    "    acc = float(n_correct / n_samples)\n",
    "    loss = float(total_loss / n_batches)\n",
    "    print(f'Untargetted MIM Test Loss: {loss:.4f}, Untargetted MIM Test Acc: {acc:.4f}')\n",
    "    return acc\n",
    "\n",
    "def eval_targetted_mim(model, test_data, loss_fn, eps=0.1, alpha=0.9, target_label=1):\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]))\n",
    "    target_label = target_label.to(device=device, dtype=torch.long)\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    momentum = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        x.requires_grad = True\n",
    "        scores = model(x)\n",
    "        loss = loss_fn(scores, target_label)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        x_grad = x.grad.data\n",
    "\n",
    "        # Update momentum and create adversarial example\n",
    "        momentum = alpha * momentum + (1 - alpha) * x_grad.sign()\n",
    "        x_adv = x - eps * momentum\n",
    "\n",
    "        # Clip adversarial example to valid range\n",
    "        x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "        out_adv = model(x_adv)\n",
    "        _, preds = out_adv.max(1)\n",
    "        total_loss += loss_fn(out_adv, y).item()\n",
    "        n_samples += preds.size(0)\n",
    "        n_correct += (preds == y).sum()\n",
    "        n_batches += 1\n",
    "\n",
    "    acc = float(n_correct / n_samples)\n",
    "    loss = float(total_loss / n_batches)\n",
    "    print(f'Untargetted MIM Test Loss: {loss:.4f}, Untargetted MIM Test Acc: {acc:.4f}')\n",
    "    return acc\n",
    "\n",
    "def eval_untargetted_rays(model, test_data, loss_fn, eps=0.1, num_iter=10, alpha=0.75):\n",
    "\n",
    "  test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "  n_samples = 0\n",
    "  n_correct = 0\n",
    "  n_batches = 0\n",
    "  total_loss = 0\n",
    "\n",
    "  for x, y in test_loader:\n",
    "    x = x.to(device=device, dtype=dtype)\n",
    "    y = y.to(device=device, dtype=torch.long)\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "      scores = model(x_adv)\n",
    "      loss = loss_fn(scores, y)\n",
    "      model.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # Get gradient and normalize\n",
    "      grad = x_adv.grad.data / torch.clamp(x_adv.grad.norm(dim=1, keepdim=True), min=1e-6)\n",
    "\n",
    "      # Momentum update\n",
    "      grad_update = alpha * grad + (1 - alpha) * x_adv - x\n",
    "\n",
    "      # Project onto feasible space\n",
    "      eta = torch.min(eps, torch.linalg.norm(grad_update, dim=1, keepdim=True))\n",
    "      x_adv = x_adv + eta * grad_update / torch.clamp(torch.linalg.norm(grad_update, dim=1, keepdim=True), min=1e-6)\n",
    "\n",
    "      # Clamp to image range\n",
    "      x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "      # Detach and require grad for next iteration\n",
    "      x_adv = x_adv.detach().requires_grad_(True)\n",
    "\n",
    "    scores_adv = model(x_adv)\n",
    "    _, preds = scores_adv.max(1)\n",
    "\n",
    "    total_loss += loss_fn(scores_adv, y).item()\n",
    "    n_samples += preds.size(0)\n",
    "    n_correct += (preds == y).sum().item()\n",
    "    n_batches += 1\n",
    "\n",
    "  acc = float(n_correct / n_samples)\n",
    "  loss = float(total_loss / n_batches)\n",
    "  print(f'Untargeted RayS Test Loss: {loss:.4f}, Untargeted RayS Test Acc: {acc:.4f}')\n",
    "  return acc\n",
    "\n",
    "def eval_targetted_rays(model, test_data, loss_fn, eps=0.1, num_iter=10, alpha=0.75,target_label=1):\n",
    "\n",
    "  test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "  target_label = target_label.to(device=device, dtype=torch.long)\n",
    "\n",
    "  n_samples = 0\n",
    "  n_correct = 0\n",
    "  n_batches = 0\n",
    "  total_loss = 0\n",
    "  for x, y in test_loader:\n",
    "    x = x.to(device=device, dtype=dtype)\n",
    "    y = y.to(device=device, dtype=torch.long)\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "      scores = model(x_adv)\n",
    "      loss = loss_fn(scores, target_label)\n",
    "      model.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # Get gradient and normalize\n",
    "      grad = x_adv.grad.data / torch.clamp(x_adv.grad.norm(dim=1, keepdim=True), min=1e-6)\n",
    "\n",
    "      # Momentum update\n",
    "      grad_update = alpha * grad + (1 - alpha) * x_adv - x\n",
    "\n",
    "      # Project onto feasible space\n",
    "      eta = torch.min(eps, torch.linalg.norm(grad_update, dim=1, keepdim=True))\n",
    "      x_adv = x_adv - eta * grad_update / torch.clamp(torch.linalg.norm(grad_update, dim=1, keepdim=True), min=1e-6)\n",
    "\n",
    "      # Clamp to image range\n",
    "      x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "      # Detach and require grad for next iteration\n",
    "      x_adv = x_adv.detach().requires_grad_(True)\n",
    "\n",
    "    scores_adv = model(x_adv)\n",
    "    _, preds = scores_adv.max(1)\n",
    "\n",
    "    total_loss += loss_fn(scores_adv, y).item()\n",
    "    n_samples += preds.size(0)\n",
    "    n_correct += (preds == y).sum().item()\n",
    "    n_batches += 1\n",
    "\n",
    "  acc = float(n_correct / n_samples)\n",
    "  loss = float(total_loss / n_batches)\n",
    "  print(f'Untargeted RayS Test Loss: {loss:.4f}, Untargeted RayS Test Acc: {acc:.4f}')\n",
    "  return acc\n",
    "\n",
    "def eval_untargetted_pgd(model, test_data, loss_fn, e=0.1, a=0.01, num_iter=30):\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        x_adv = x.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        for t in range(num_iter):\n",
    "            scores = model(x_adv)\n",
    "            loss = loss_fn(scores, y)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            x_adv_grad = x_adv.grad.data\n",
    "            x_adv = x_adv + a * x_adv_grad.sign()\n",
    "            x_adv = torch.min(torch.max(x_adv, x - e), x + e)\n",
    "            x_adv = torch.clamp(x_adv, 0, 1)\n",
    "            x_adv = x_adv.detach().requires_grad_(True)\n",
    "        \n",
    "        scores_adv = model(x_adv)\n",
    "        _, preds = scores_adv.max(1)\n",
    "        \n",
    "        total_loss += loss_fn(scores_adv, y).item()\n",
    "        n_samples += preds.size(0)\n",
    "        n_correct += (preds == y).sum().item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    acc = float(n_correct / n_samples)\n",
    "    loss = float(total_loss / n_batches)\n",
    "    print(f'Untargeted PGD Test Loss: {loss:.4f}, Untargeted PGD Test Acc: {acc:.4f}')\n",
    "    return acc\n",
    "     \n",
    "def eval_targeted_pgd(model, test_data, loss_fn, target_label, e=0.1, a=0.01, num_iter=30):\n",
    "    test_loader = DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        target_label = target_label.to(device=device, dtype=torch.long)\n",
    "        x_adv = x.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        for t in range(num_iter):\n",
    "            scores = model(x_adv)\n",
    "            # loss1 = loss_fn(scores, y)\n",
    "            # loss2 = loss_fn(scores, target_label)\n",
    "            # loss = loss1-loss2\n",
    "            loss = loss_fn(scores, target_label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            x_grad = x_adv.grad.data\n",
    "            \n",
    "            # Perturb the input with PGD\n",
    "            x_adv = x_adv - a * x_grad.sign()\n",
    "            x_adv = torch.min(torch.max(x_adv, x - e), x + e)\n",
    "            x_adv = torch.clamp(x_adv, 0, 1)\n",
    "            x_adv = x_adv.detach().requires_grad_(True)\n",
    "        \n",
    "        scores_adv = model(x_adv)\n",
    "        loss_adv = loss_fn(scores_adv, y)\n",
    "        _, preds = scores_adv.max(1)\n",
    "        \n",
    "        total_loss += loss_adv.item()\n",
    "        n_samples += preds.size(0)\n",
    "        n_correct += (preds == y).sum().item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    acc = float(n_correct / n_samples)\n",
    "    loss = float(total_loss / n_batches)\n",
    "    print(f'Targeted PGD Test Loss: {loss:.4f}, Targeted PGD Test Acc: {acc:.4f}')\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now performing adversarial training on the model and checking the accuracies again\n",
    "\n",
    "def adversarial_training(model, train_data, test_data, loss_fn, e=0.05, a=0.01, num_iter=20, num_epochs=10, batch_size=32):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            x.requires_grad = True\n",
    "            # 25% samples are adversarial\n",
    "            if np.random.rand() < 0.05:\n",
    "                for t in range(num_iter):\n",
    "                    scores = model(x)\n",
    "                    loss = loss_fn(scores, y)\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    x_grad = x.grad.data\n",
    "                    x_adv = x + a * x_grad.sign()\n",
    "                    x_adv = torch.min(torch.max(x_adv, x - e), x + e)\n",
    "                    x_adv = torch.clamp(x_adv, 0, 1)\n",
    "                    x = x_adv.detach().requires_grad_(True)\n",
    "            train_preds = model(x)\n",
    "            loss = loss_fn(train_preds, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += loss.item()\n",
    "        loss /= len(train_loader)\n",
    "        train_acc, _ = check_acc(train_loader, model, loss_fn)\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    \n",
    "    print(\"Clean Accuracy:\")\n",
    "    eval_model(model, test_data, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "intermed_len = int(len(train_data) * 0.25)\n",
    "robust_len = len(train_data) - intermed_len\n",
    "intermed_train, robust_train = random_split(train_data, [intermed_len, robust_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.4084, Train Acc: 0.6847\n",
      "Epoch 1, Train Loss: 0.9529, Train Acc: 0.7438\n",
      "Epoch 2, Train Loss: 0.7297, Train Acc: 0.8360\n",
      "Epoch 3, Train Loss: 0.5419, Train Acc: 0.8598\n",
      "Epoch 4, Train Loss: 0.4607, Train Acc: 0.9041\n",
      "Epoch 5, Train Loss: 0.3776, Train Acc: 0.9261\n",
      "Epoch 6, Train Loss: 0.3029, Train Acc: 0.9412\n",
      "Epoch 7, Train Loss: 0.2961, Train Acc: 0.9452\n",
      "Epoch 8, Train Loss: 0.2122, Train Acc: 0.9618\n",
      "Epoch 9, Train Loss: 0.1459, Train Acc: 0.9670\n",
      "Epoch 10, Train Loss: 0.2650, Train Acc: 0.9264\n",
      "Epoch 11, Train Loss: 0.1828, Train Acc: 0.9618\n",
      "Epoch 12, Train Loss: 0.1344, Train Acc: 0.9687\n",
      "Epoch 13, Train Loss: 0.0957, Train Acc: 0.9775\n",
      "Epoch 14, Train Loss: 0.0960, Train Acc: 0.9810\n",
      "Epoch 15, Train Loss: 0.0901, Train Acc: 0.9761\n",
      "Epoch 16, Train Loss: 0.0933, Train Acc: 0.9806\n",
      "Epoch 17, Train Loss: 0.0920, Train Acc: 0.9751\n",
      "Epoch 18, Train Loss: 0.0986, Train Acc: 0.9775\n",
      "Epoch 19, Train Loss: 0.1043, Train Acc: 0.9773\n",
      "Epoch 20, Train Loss: 0.1142, Train Acc: 0.9572\n",
      "Epoch 21, Train Loss: 0.1824, Train Acc: 0.9809\n",
      "Epoch 22, Train Loss: 0.0728, Train Acc: 0.9905\n",
      "Epoch 23, Train Loss: 0.0318, Train Acc: 0.9930\n",
      "Epoch 24, Train Loss: 0.0880, Train Acc: 0.9758\n",
      "Epoch 25, Train Loss: 0.0902, Train Acc: 0.9674\n",
      "Epoch 26, Train Loss: 0.0900, Train Acc: 0.9902\n",
      "Epoch 27, Train Loss: 0.0462, Train Acc: 0.9874\n",
      "Epoch 28, Train Loss: 0.0595, Train Acc: 0.9854\n",
      "Epoch 29, Train Loss: 0.4933, Train Acc: 0.9024\n",
      "Epoch 30, Train Loss: 0.1471, Train Acc: 0.9882\n",
      "Epoch 31, Train Loss: 0.0470, Train Acc: 0.9850\n",
      "Epoch 32, Train Loss: 0.0309, Train Acc: 0.9931\n",
      "Epoch 33, Train Loss: 0.0337, Train Acc: 0.9929\n",
      "Epoch 34, Train Loss: 0.0215, Train Acc: 0.9954\n",
      "Epoch 35, Train Loss: 0.0258, Train Acc: 0.9903\n",
      "Epoch 36, Train Loss: 0.0826, Train Acc: 0.9766\n",
      "Epoch 37, Train Loss: 0.0858, Train Acc: 0.9903\n",
      "Epoch 38, Train Loss: 0.0465, Train Acc: 0.9938\n",
      "Epoch 39, Train Loss: 0.0289, Train Acc: 0.9950\n",
      "Epoch 40, Train Loss: 0.0361, Train Acc: 0.9904\n",
      "Epoch 41, Train Loss: 0.0571, Train Acc: 0.9920\n",
      "Epoch 42, Train Loss: 0.0667, Train Acc: 0.9904\n",
      "Epoch 43, Train Loss: 0.0355, Train Acc: 0.9932\n",
      "Epoch 44, Train Loss: 0.0308, Train Acc: 0.9912\n",
      "Epoch 45, Train Loss: 0.0593, Train Acc: 0.9942\n",
      "Epoch 46, Train Loss: 0.0265, Train Acc: 0.9938\n",
      "Epoch 47, Train Loss: 0.0370, Train Acc: 0.9913\n",
      "Epoch 48, Train Loss: 0.0222, Train Acc: 0.9937\n",
      "Epoch 49, Train Loss: 0.0458, Train Acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "rst_model = MyModel(10).to(device=device)\n",
    "from torchvision.models.resnet import resnet18 as _resnet18\n",
    "\n",
    "# test_model = _resnet18(pretrained=True,).to(device=device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_model(rst_model, intermed_train, test_data, loss_fn, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4298, Test Acc: 0.7346\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataset, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate predictions for a given dataset using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model.\n",
    "    - dataset: Dataset for which to predict labels. Should be a DataLoader.\n",
    "    - device: Device to use for computation ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of predictions.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for data in dataset:\n",
    "            inputs = data[0].to(device)  # Assumes data is a tuple of (inputs, labels)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            predictions.extend(predicted.cpu().tolist())\n",
    "\n",
    "    return torch.tensor(predictions)  # Convert list of predictions to a tensor\n",
    "\n",
    "# Create DataLoader for the remainder dataset\n",
    "robust_loader = DataLoader(robust_train, batch_size=75, shuffle=False)\n",
    "\n",
    "# # Assuming your model and remainder_loader are defined, and you've set the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predictions = get_predictions(rst_model, robust_loader, device=device)\n",
    "\n",
    "eval_model(rst_model, robust_train, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "pseudo_data = torch.stack([data for data, _ in robust_train])\n",
    "\n",
    "class PseudoLabeledDataset(Dataset):\n",
    "    def __init__(self, data, pseudo_labels):\n",
    "        \"\"\"\n",
    "        A dataset wrapping tensors of data and pseudo-labels.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (Tensor): The data points.\n",
    "        - pseudo_labels (Tensor): The pseudo-labels for the data points.\n",
    "        \"\"\"\n",
    "        assert data.size(0) == pseudo_labels.size(0), \"Data and labels must have the same size\"\n",
    "        self.data = data\n",
    "        self.labels = pseudo_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "class TensorLabelDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "        return data, torch.tensor(label)\n",
    "\n",
    "intermed_train = TensorLabelDataset(intermed_train)\n",
    "pseudo_labeled_dataset = PseudoLabeledDataset(pseudo_data, predictions)\n",
    "\n",
    "final_dataset = ConcatDataset([pseudo_labeled_dataset, intermed_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fgsm_attack(model, loss_fn, images, labels, epsilon):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates adversarial examples using the Fast Gradient Sign Method.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model.\n",
    "    - loss_fn: Loss function used to compute gradients.\n",
    "    - images: Original images.\n",
    "    - labels: True labels for the images.\n",
    "    - epsilon: Perturbation magnitude.\n",
    "\n",
    "    Returns:\n",
    "    - Adversarial examples.\n",
    "    \"\"\"\n",
    "\n",
    "    images = images.clone().detach().requires_grad_(True).to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # print(images.shape)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    images.requires_grad_(True)\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    adv_images = images + epsilon * images.grad.sign()\n",
    "    adv_images = torch.clamp(adv_images, 0, 1)  # Ensure the perturbed images are valid\n",
    "\n",
    "    return adv_images\n",
    "\n",
    "class robust_loss():\n",
    "    \"\"\"\n",
    "    Computes the robust loss function, combining cross-entropy with a regularization term.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model.\n",
    "    - inputs: Input images.\n",
    "    - labels: True labels for the images.\n",
    "    - epsilon: Perturbation magnitude for adversarial examples.\n",
    "    - alpha: Weighting factor for the regularization term.\n",
    "\n",
    "    Returns:\n",
    "    - The computed loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataset, epsilon=0.03, alpha=0.5):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_inputs_for_label(self, target_label):\n",
    "        matching_images = []\n",
    "        found_labels = {}\n",
    "        for data, label in self.dataset:\n",
    "            if label in target_label and label not in found_labels:\n",
    "                matching_images.append(data.unsqueeze(0))  # Add an extra dimension to match the batch dimension\n",
    "                found_labels[label] = True\n",
    "            if len(found_labels) == len(target_label):  # If we've found a match for each label\n",
    "                break\n",
    "        if not matching_images:  # If no matching images were found\n",
    "            return None\n",
    "        return torch.cat(matching_images, 0).to(device)  # Concatenate along the batch dimension and move to the correct device\n",
    "\n",
    "    def robust_loss_fn(self, train_preds, y):\n",
    "\n",
    "        # Standard cross-entropy loss\n",
    "        train_preds = train_preds.to(device)\n",
    "        y = y.to(device)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        std_loss = loss(train_preds, y)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        adv_inputs = fgsm_attack(self.model, nn.CrossEntropyLoss(), self.get_inputs_for_label(y), y, self.epsilon)\n",
    "\n",
    "        # Compute loss for adversarial examples\n",
    "        adv_outputs = self.model(adv_inputs)\n",
    "        adv_loss = F.cross_entropy(adv_outputs, y)\n",
    "\n",
    "        # Combine standard loss with adversarial loss\n",
    "        combined_loss = (1 - self.alpha) * std_loss + self.alpha * adv_loss\n",
    "\n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(intermed_train[0][1], pseudo_labeled_dataset[0][1])\n",
    "\n",
    "\n",
    "rst_model = rst_model.to(device)\n",
    "loss_fn = robust_loss(model = rst_model, dataset = final_dataset)\n",
    "adversarial_training(rst_model, final_dataset, test_data, loss_fn.robust_loss_fn, e=0.1, a=0.01, num_iter=30, num_epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
