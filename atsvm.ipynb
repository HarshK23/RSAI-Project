{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T=10\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Auxiliary_Classifier:\n",
    "    def __init__(self, C, k, n_features):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.k = k  # Number of classes\n",
    "        self.omega = np.random.rand(k, n_features)  # Weights\n",
    "        self.b = np.random.rand(k)  # Bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        L = np.zeros((n_samples, self.k))\n",
    "        mask = np.zeros((n_samples, self.k))\n",
    "\n",
    "        print(\"SVM Shapes\", self.omega.T.shape, X[0].shape, self.b.shape, y.shape)\n",
    "\n",
    "        # Compute the loss matrix L and the mask\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.k):\n",
    "                if j != y[i]:\n",
    "                    margin = self.omega[y[i], j] @ X[i] - self.omega[y[i]] @ X[i] + self.b[j] - self.b[y[i]]\n",
    "                    L[i, j] = max(0, 1 - margin)\n",
    "                    if L[i, j] > 0:\n",
    "                        mask[i, j] = 1\n",
    "\n",
    "        # print(X.shape, mask.shape, L.shape, y.shape)\n",
    "\n",
    "        # Gradient update for weights and bias\n",
    "        for j in range(self.k):\n",
    "            if j in y:\n",
    "                # Update rule for class j\n",
    "                indices = np.where(y == j)[0]\n",
    "                omega_grad = np.sum(X[indices][:, np.newaxis, :] * mask[indices][:, :, np.newaxis], axis=0)\n",
    "                self.omega[j] -= omega_grad.mean(axis=0) - self.C * self.omega[j]\n",
    "\n",
    "                # Update rule for bias j\n",
    "                b_grad = mask[indices].sum(axis=0)\n",
    "                self.b[j] -= b_grad.mean() - self.C * self.b[j]\n",
    "\n",
    "        return mask, L\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by selecting the class with the highest decision function value\n",
    "        decision_function = X @ self.omega.T + self.b\n",
    "        return np.argmax(decision_function, axis=1)\n",
    "    \n",
    "# Dummy data for demonstration purposes\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# X_dummy = np.random.randn(100, 10)  # 100 samples, 10 features\n",
    "\n",
    "# y_dummy = np.random.choice([1, -1], size=100)  # Binary classification problem\n",
    "\n",
    "# # Create an instance of SVM_Auxiliary_Classifier with dummy parameters\n",
    "# svm_aux = SVM_Auxiliary_Classifier(C=1, k=2, n_features=10)\n",
    "\n",
    "# # Fit the classifier to the dummy data\n",
    "# mask, L = svm_aux.fit(X_dummy, y_dummy)\n",
    "\n",
    "# # Output the mask and loss matrix for verification\n",
    "# mask, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "torch.Size([1250, 512, 1, 1])\n",
      "SVM Shapes (1250, 10) (512,) (10,) (1250,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train SVM auxiliary classifier\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m mask, L_margin \u001b[38;5;241m=\u001b[39m \u001b[43msvm_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compute loss L_margin\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 19\u001b[0m, in \u001b[0;36mSVM_Auxiliary_Classifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m y[i]:\n\u001b[0;32m---> 19\u001b[0m         margin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39momega[y[i]] \u001b[38;5;241m@\u001b[39m X[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[j] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[y[i]]\n\u001b[1;32m     20\u001b[0m         L[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m margin)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m L[i, j] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "\n",
    "# Assuming the SVM_Auxiliary_Classifier is defined as above\n",
    "\n",
    "def pgd_attack(image, epsilon, data_grad, num_steps=10, step_size=2/255):\n",
    "    # Initialize the perturbed image as a copy of the original image\n",
    "    perturbed_image = image.clone().detach()\n",
    "    \n",
    "    # Iteratively apply gradient ascent for the specified number of steps\n",
    "    for _ in range(num_steps):\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        # Add a small step in the direction of the sign of the gradient\n",
    "        perturbed_image += step_size * sign_data_grad\n",
    "        # Clip the perturbed image to stay within the epsilon budget\n",
    "        perturbed_image = torch.clamp(perturbed_image, image - epsilon, image + epsilon)\n",
    "        # Clip the perturbed image to ensure it stays within the valid image range [0, 1]\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        \n",
    "    return perturbed_image.detach().to(\"cpu\").numpy()\n",
    "\n",
    "# Functions for generating PGD attacks and computing loss L_margin\n",
    "# These should be defined according to the problem specification\n",
    "def generate_pgd_attacks(model, images, labels):\n",
    "    # This function should generate and return adversarial examples using PGD\n",
    "    perturbed_images = []\n",
    "    images.requires_grad = True\n",
    "    output = model(images)\n",
    "    loss = criterion(output, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = images.grad.data\n",
    "    for i in range(images.shape[0]):\n",
    "        perturbed_images.append(pgd_attack(images[i], 8/255, data_grad[i]))\n",
    "    \n",
    "    return np.array(perturbed_images)\n",
    "\n",
    "def compute_loss_lmargin(output, labels, mask, svm_classifier):\n",
    "    # This function should compute and return the L_margin loss\n",
    "    pass\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=625, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(test_dataset, batch_size=625, shuffle=False, drop_last=True)\n",
    "\n",
    "# Initialize ResNet-18 network\n",
    "resnet18 = models.resnet18(pretrained=False, num_classes=10)\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(resnet18.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=75, gamma=0.1)\n",
    "\n",
    "print(resnet18.fc.in_features)\n",
    "\n",
    "# Training process\n",
    "svm_classifier = SVM_Auxiliary_Classifier(C=1, k=10, n_features=trainloader.batch_size * 2)\n",
    "resnet18.train()\n",
    "# print(\"ghello\")\n",
    "\n",
    "for epoch in range(T):\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # Generate PGD attack examples\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # print(inputs.shape)\n",
    "\n",
    "        adv_inputs = torch.tensor(generate_pgd_attacks(resnet18, inputs, labels))\n",
    "        adv_inputs = adv_inputs.to(device)\n",
    "\n",
    "        total_inputs = torch.cat((adv_inputs, inputs), 0)\n",
    "\n",
    "        labels = torch.cat((torch.tensor([0] * labels.shape[0]).to(device), labels), 0)\n",
    "\n",
    "        # print(adv_inputs.shape, inputs.shape)\n",
    "\n",
    "        # print(\"PGD attack done\", type(adv_inputs))\n",
    "\n",
    "        adv_inputs = adv_inputs.to(device)\n",
    "\n",
    "        # Get features before the last FC layer\n",
    "        features = feature_extractor(total_inputs)\n",
    "        print(features.shape)\n",
    "\n",
    "        # Train SVM auxiliary classifier\n",
    "        mask, L_margin = svm_classifier.fit(features.cpu().detach().squeeze().numpy(), labels.cpu().numpy())\n",
    "\n",
    "        print(mask.shape)\n",
    "\n",
    "        # Compute loss L_margin\n",
    "        outputs = resnet18(total_inputs)\n",
    "        # L_margin = compute_loss_lmargin(outputs, labels, mask, svm_classifier)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        outputs = outputs.to(device)\n",
    "        # print(\"Shapes: \", labels.shape, outputs.shape, mask.shape, L_margin.shape)\n",
    "\n",
    "        # Compute the masked cross-entropy loss\n",
    "        loss = (criterion(outputs, labels) * torch.from_numpy(mask).float().to(device)).mean()\n",
    "\n",
    "        loss = loss.to(device)\n",
    "        L_margin = torch.tensor(L_margin).float().to(device)\n",
    "\n",
    "        total_loss = (loss + L_margin).mean().clone().detach().requires_grad_(True)\n",
    "        # total_loss = total_loss.requires_grad_(True)\n",
    "        # print(total_loss, loss, L_margin.mean())\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{T} finished, Loss: {total_loss.item()}')\n",
    "\n",
    "print('Finished Training. SVM auxiliary classifier will be removed for inference.')\n",
    "\n",
    "resnet18.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = resnet18(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
